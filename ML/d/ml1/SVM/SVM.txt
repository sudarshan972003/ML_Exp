import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm

# Generating random linear data for two classes (A and B)
np.random.seed(42)
class_A = np.random.randn(50, 2) - [2, 2]
class_B = np.random.randn(50, 2) + [2, 2]

# Concatenating the data
X = np.vstack((class_A, class_B))
y = np.hstack((np.zeros(50), np.ones(50)))  # Class labels: 0 for A, 1 for B

# Training an SVM model with a linear kernel
clf = svm.SVC(kernel='linear', C=1.0)
clf.fit(X, y)

# Get the separating hyperplane
w = clf.coef_[0]
b = clf.intercept_[0]
slope = -w[0] / w[1]
intercept = -b / w[1]

# Plotting the decision boundary, support vectors, and hyperplanes
plt.figure(figsize=(10, 6))

# Create grid to evaluate model
xx = np.linspace(X[:, 0].min()-1, X[:, 0].max()+1, 500)
yy = slope * xx + intercept

# Plot decision boundary
plt.plot(xx, yy, 'k-', label='Decision Boundary')

# Plot margin hyperplanes
margin = 1 / np.sqrt(np.sum(clf.coef_ ** 2))
yy_pos = yy + margin
yy_neg = yy - margin
plt.plot(xx, yy_pos, 'k--', label='Positive Hyperplane')
plt.plot(xx, yy_neg, 'k--', label='Negative Hyperplane')

# Plot the data points
plt.scatter(class_A[:, 0], class_A[:, 1], color='r', label='Class A')
plt.scatter(class_B[:, 0], class_B[:, 1], color='b', label='Class B')

# Plot support vectors
plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],
            s=100, facecolors='none', edgecolors='k', label='Support Vectors')

plt.title("SVM with Linear Kernel")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.legend()
plt.show()
